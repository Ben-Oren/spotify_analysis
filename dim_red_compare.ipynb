{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link for outline\n",
    "<a id='dim_red_compare'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we trying to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out ways to compare dimensionality reduction schemes of \"spotify-provided features\" vs \"raw data\" of songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brainstorm ideas\n",
    "\n",
    "- There is both \"list of spotify-provided song varibles\" and \"raw song data eg start/duration/confidence of bars, beats, etc\".  Idea is to: \n",
    "\n",
    "    - put together aggregated statistics of bars-beats-etc raw data for a song\n",
    "\n",
    "    - do PCA reconstruction error with each dataset to see if we can tell 'what songs are most like the playlist' or something.  Metric for 'what songs are most like the playlist' maybe something like \"lowest PCA reconstruction error\" or maybe \"look at KNN distances for songs with lowest PCA R E\".\n",
    "    \n",
    "- Compare the datasets for \"how well do they make songs in a playlist 'pop out' vs songs not in that playlist\" (supervised task)\n",
    "    - Idea is that this is quick 'n dirty, so don't go all out modeling crazy\n",
    "    \n",
    "    - Use descriptive statistics and see which comparison of \"in playlist vs not\" is more drastic \n",
    "    \n",
    "    - use quick 'n dirty knn supervised task of \"is this song in the playlist or not\". why knn?  quick 'n dirty, but also: this dim reduction is creating a specific topographical space vs \"clusters we turn into features\", so supervised task that uses topographical space better than eg naive bayes.  I dunno use naive bayes too just to see?  Quick 'n dirty\n",
    "    \n",
    "    - stretch goal: see which dataset does better at 'goal of PCA', minimizing sum of squared residual errors between projected data points and original data.  Outlined on pg 5 and 22 here: https://dl.acm.org/doi/pdf/10.5555/2789272.2912091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
